\input respnotes
\input xpmath
\input unifonts \tenrm

\def\cite#1{{\tt #1}}

\title Log of Bogoliubov dynamics work for Tapio

2019-06-24 Overview meeting

I'm going to write some software for Tapio.  This meeting outlined
what it will do.

The initial goal is to solve the BdG equations statically, in 2D,
with an arbitrary potential.  This will be a simple prototype to
validate the faster and more complicated versions I'll develop
later.  I already have parts of it in Matlab.  The algorithm is as
follows (at least for bosons).

\item{1.} Solve very precisely for the equilibrium order parameter.
The Bogoliubov modes are very sensitive to its chemical potential.
Tapio knows an efficient way to do this, called Successive Over Relaxation.

\item{2.} Set up a whopping great numerical derivative matrix.

\item{3.} Diagonalise.  We want all the eigenfunctions, so it isn't
clear that Krylov subspace methods will do better than dense methods.

No doubt there are some tricks, e.g.\ the Householder reflection
to keep the excited modes orthogonal to the order parameter.

My code to do Steps~2 and~3 is at {\tt https://github.com/thisrod/drugs/blob/master/bec/buv.m}.

Later, the idea is to propagate the sound wave modes in parallel.
Apparently each mode follows GPE dynamics, coordinated by a common
density.  Bogdan has code to solve lots of GPEs in parallel for
Wigner and positive-P calculations.  We plan to work in Julia, which
is no doubt a better alternative nowadays compared to Bodgan's hand
kludged macros.  However, that code will be at least a useful resouce
to look up how to go fast.  Possibly, the shortest path to our goal
is to port the Wigner code to Julia, then modify it to propagate
Bogoliubov modes instead of phase-space samples.

2019-06-25 Outline of project

The initial goal is for me to play around with BdG diagonalisation
in Matlab.  This has two goals.  The first is Fred Brooks's “build
one to throw away.”  The second is to have a simple and correct
prototype, to validate the complicated ones that I'm going to build
later.

Imaginary time propagation is a simple way to find the equilibrium
order parameter, but it has limitations described below on 2019-06-26.  The Successive Over Relaxation method avoids them.  Instead of
propagating the whole order parameter in imaginary time, you update
one grid point at once.  There is some trick, called over relaxation,
to preserve the norm.  I guess this is calculating the GPE operator
on a normalised state, so when it converges, it has the exact
solution.  I'll need to read some papers to figure out the details.

Once the order parameter has converged and the propagation has
stopped, it is a good idea to test the residual of the GPE.  This
will show up any systematic errors of that type.

Tapio doesn't think that the basic Bogoliubov approach will suffice.
He wants to use a method that is self-consistent with respect to
the non-condensate density, such as Hartree-Fock-Bogoliubov or a
Popov method.

There are a few methods to validate a BdG solver.  The first check
is that it reproduces the condensate mode, with a pair of energies~$±E$
due to the~$u→v*$ symmetry.  The energy~$E$ should be small, on the
order of~$10^{-10}·ℏω$.

The next step is to check that the solver correctly reproduces the
modes for a harmonic trap, with and without a soliton.  The simplest
check is a plot of mode energy against angular momentum.  You can
calculate angular momentum by taking expectation values of the mode
functions.  (Except that there are two mode functions, $u$ and~$v$.
Do they necessarily have the same angular momentum?  Probably this
is necessary for the angular momentum of the order parameter to be
conserved as the sound waves oscillate.)

The harmonic oscillator, with zero repulsion, should exactly reproduce
the eigenstates, with the~$nℏω$ ladder.  What does angular momentum
do for a 2D oscillator?  Small repulsion should cause small changes
in the eigenvalues.

A vortex order parameter can be found by solving the GPE with a
Lagrange multiplier, just like the chemical potential can set the
particle number.  This means solving
$$(-∇²+g|ψ|²-Ω\hat L_z)ψ=μψ.$$
I'll need to think through the details of that.  How do you set~$Ω$?
I guess this can't give a superposition of angular momenta because
there will be a unique value of~$L_z$ that minimises the energy.

The vortex has a Kelvin mode.  This has angular momentum~$-ℏ$, and
negative energy.  (The degenerate pairs of modes have opposite
energy and angular momentum.)  The negative energy means that you
can reduce the energy of the gas by exciting the mode.  It rotates
against the sense of the vortex, and represents the vortex core
spiralling outwards and escaping from the edge of the trap.  This
mode doesn't matter physically because it breaks conservation of
angular momentum, but it provides a useful numerical check.

The key numerical issue is how to implement the linear operator
defined by the effect on~$(u_j,v_j)$ of the left-hand side of
Equations~2.21 in \cite{aop-70-67}.  Apparently, the time evolution
of the modes can be found by replacing the right hand sides of those
Equations by~$du\over dt$ and~$dv\over dt$.  So the operator that
must be diagonalised to give the initial modes is the same one that
must be applied to propagate dynamics.

A tempting approach is to parallelise the application of this
operator, using a Finite Discrete Variable(?) method.  This breaks
the 2D grid into a bunch of subgrids, does a spectral derivative
on each subgrid, and enforces continuity at the boundaries.  The
resulting matrix is almost block diagonal, each block correponding
to a subgrid.  The exception is that the last row and column of
each block overlap with the first row and column of the next one,
to enforce the boundary condition.

The parallelism from FDV comes from evaluating the blocks independently
on the subgrids.  Plausibly, each block could be assigned to a GPU,
and applied to~$10⁴$ mode vectors in parallel by GPU threads.  The
same parallel linear operator can be used both for diagonalisation
at the start, and for time propagation.

The other approach is to propagate the mode vectors by solving lots
of GPEs in parallel, as in a phase space simulation.

We need to look at the resources available on OzStar, and see if
there is enough memory to keep all the modes in core at once.  The
minimum useful grid is~$100×100$, with~$10⁴$ points.  Almost all
of the BdG modes need to be accounted for, so the memory required
for double precision is~$(10⁴\,{\rm modes})×(10⁴\,{\rm
points/mode})×(8\,\rm B)=1\,\rm GB$.  Tapio would like to use larger
grids where this comes out to~$1\,\rm TB$.

If all the modes fit into memory at once, the FDV parallelism makes
sense.  Otherwise, modes will have to be swapped in and out of
memory.  The problem isn't embarassingly parallel, because every
mode depends on the total particle density, which depends on all
the modes.

Network complexity shouldn't be an issue.  Only the total density
has to be communicated, and the boundary conditions if the FDV
parallelism is exploited.

2019-06-26 Normalising imaginary time propagation

Imaginary time propagation is a standard way to find the equilibrium
order parameter of a Bose gas.  Let's say for now that the GPE is
$$i{ψ_t}=(-∇²+V+g|ψ|²)ψ,$$
where~$V$ is a trapping potential.  A substitution is made with~$τ=it$, so the GPE becomes
$${ψ_τ}=-(-∇²+V+g|ψ|²)ψ.$$
When this is propagated over~$τ$, the order parameter~$ψ(x,t)$
shrinks over time, but the high-energy components shrink faster
than the low-energy ones.  If the order parameter is renormalised
during the propagation, to keep~$∥ψ∥²=N$, then the solution will
converge to the equilibrium order parameter~$ψ₀$.

The obvious way to discretise this is to solve the GPE for a time
step~$h$, to get~$ψ₁(x)$, then set~$ψ(x,t+h)=√N·ψ₁(x)/∥ψ₁∥$.  The
propagation stops when the $ψ(x,t+h)=ψ(x,t)$.  However, during the
time step, the GPE is being solved with a reduced density, which
makes the converged chemical potential systematically low.  In
principle, you could fix this by reducing the time step over the
propagation, but that becomes inefficient in practice.

Peter Drummond suggested another way to renormalise.  Let~${\cal
G}ψ=-(-∇²+V+g|ψ|²)ψ$ be the right-hand side of the GPE.  Then,
instead of discontinuously updating the order parameter each time
step, subtract the component parallel to~$ψ$, to give a modified
GPE
$${ψ_τ}=\left({\cal G}-∫{ψ*\over ∥ψ∥}{\cal G}ψ\right)ψ.$$
The original motivation was that higher order PDE solvers rely
on~$ψ$ being a continuous function of~$t$, and lose precision due
to the jumps in normalisation.  But there's another benefit relevant
here.  The solution to this equation preserves its particle number
over each time step, and might avoid the systematic low density
problem.

In the RK4IP algorithm, the operator is split as~${\cal G}=D+N$.
By linearity, subtracting the parallel components from~$D$ and~$N$
separately adds up to subtract them from~$\cal G$.

2019-06-26 Checking time steps and convergence

The naive imaginary time method (and maybe other methods) has a
time step adaptation problem.  We want to detect when the chemical
potential~$μ$ is close to convergence, but the residual of the GPE
is large.  Idea: find the current chemical potential, estimate the
converged value by whatsisname extrapolation, find the residual,
then reduce the time step if the residual is large compared to the
difference between the two chemical potentials.

2019-07-02 Heat equation validation

The RK4IP algorithm, in imaginary time, involves the operator~$\exp(-τD)$,
where~$D=-∇²$.  On its own, this operator solves the heat
equation~$φ_τ=-Dφ$, with~$φ(x,τ)=\exp(-τD)φ(x,0)$ up to abuse of
notation.  That equation is separable, and it has the 1D solution
$$φ(x,t)={1\over 2√{πt}}e^{-x²\over 4t}.$$
The implementation of~$\exp(-τD)$ should
satisfy~$φ(x,t+τ)=\exp(-τD)φ(x,t)$.

2019-07-05 Trick for finding convergence

The following plot shows a numerical process converging to some
chemical potential~$μ$.  The process terminated before convergence,
and the final value of~$μ$ has been subtracted.  For the initial
steps, where the final value might as well be the exact value, the
graph shows a nice geometric convergence, gaining an extra digit
every 50 steps.  This appears as a straight line on the semi-log
plot.

If the exact converged value had been subtracted, this straight
line would continue.  As it is, there comes a point where~$μ$ has
converged to within about~$10^{-4}$ of final value, which is about
as far as the final value is from the converged value.  At this
point, the difference matters.  The graph drops below the line,
because the values are significantly closer to the final value than
the converged value.

The trick is that, just by looking at the graph, I can tell that~$μ$ is
about~$10^{-4}$ away from convergence.

Is there a way to extrapolate geometric convergence?

\XeTeXpicfile resp190705.png width 0.7\hsize

2019-07-05 Gaussians

The normal distribution with standard deviation~$σ(t)$ is
$$φ(x,t) = {e^{-x²/2σ²}\over √{2π}σ}.$$
This satisfies
$$φ_t = \left({x²\over σ²}-1\right){φ\over σ}\dot σ
	\qquad{\rm and}\qquad
	φ_{xx} = \left({x²\over σ²}-1\right){φ\over σ²}.
$$
The 1D harmonic oscillator Hamiltonian, in the scaled units that I'm using, is~$=-D_{xx}+x²$, so the gaussian eigenstate has~$σ=1$, and eigenvalue~1.  Normalisation gives~$φ₀(x)=π^{-1\over 4}e^{-x²/2}$.  The 2D ground state is~$φ₀(x,y)=φ₀(x)·φ₀(y)={1\over √π}e^{-(x²+y²)/2}$, with eigenvalue~2.

To solve the heat equation~$φ_t=φ_{xx}$, set~$\dot σ={1\over σ}$, with solution~$σ(t)=√{2t}$.

2019-07-08 Successive over-relaxation

The SOR method is a way to solve a system of linear equations,
$Ay=b$.  (I'll call the unknown~$y$, because in my case it is a
vector of samples from the wave function~$ψ(x_i)$, and I want to
save~$x$ for the coordinate.)  SOR is a variation of an interative
method called Gauss-Seigel.  The idea is to start with a guess~$y^{(0)}$,
then iterate over the grid~$x$, so that we currently have updated
estimates~$y^{(1)}_j$ for~$1≤j<i$.  We now pick row~$i$, and solve
the equation
$$b_i = ∑_{j=1}^{i-1}a_{ij}y^{(1)}_j + a_{ii}y_i + ∑_{j=i+1}^{n}a_{ij}y^{(0)}_j$$
for the unknown~$y_i$.

The modification in SOR is that, instead of using~$y^{(1)}$ as the
next estimate for~$y$, we use an extrapolation~$ay^{(0)}+(1-a)y^{(1)}$
instead.  There are two ways to do this.  The first, extrapolating
the whole grid, is as written.  The second, pointwise extrapolation,
takes updated estimates~$y^{(1)}_j$ for~$1≤j<i$ and old
extimates~$y^{(0)}_j$ for~$i<j≤n$, solves for the unknown~$y_i$,
and sets~$y_i^{(1)}=y_i^{(0)}+a(y_i-y_i^{(0)})$.  The latter is
used in practice.  Applying a small extrapolation parameter~$a≈1.4$
at every point can greatly accelerate convergence.

Books have been written about the stability and convergence of all
this, but I'm going to ignore that for now.  The initial goal is
to find equilibrium order parameters by Gauss-Siegel.

There are two complications.  First, the Gross-Pitaevskii problem
is noninear.  On the left-hand side, we have~$A(y)y$ instead of
just~$Ay$.  I'll deal with that by using~$A(y^{(0)})$ to
calculate~$y^{(1)}$.  The obvious alternative would be to
use~$\pmatrix{y^{(1)}₁&…&y^{(1)}_{i-i}&y^{(0)}_i&…&y^{(0)}_n}$ to
compute~$y^{(1)}_i$.

The more serious complication is the right-hand side.  Instead of
a fixed target~$b$, I have to aim at a moving target~$μy$.  From
one of Tapio's papers \cite{cpc-142-396}, it sounds like the answer
is to fix~$μ$, solve for~$y$, find the particle number, update~$μ$,
and keep trying until you get the right number of particles.  However,
the problem~$(A-μI)y=0$ usually has as its only solution~$y=0$,
which isn't much help.  I'll start by solving~$(A(y^{(0)})y =
μy^{(0)}$ for~$y^{(1)}$, and update~$μ$ once that has converged.

2019-07-09 Spectral derivatives

When taking the spectral derivative of a whole field, it's more efficient to use Fourier transforms.  The Gauss-Seidel algorithm evaluates it one point at a time, so explicit derivative matrices win there.

Trefethen derives the spectral derivative matrix for a special case, where the domain is~$[0,2π]$, the grid is~$x_j=2πj/N$, $j=1,2,⋯,N$, and~$N$ is even.The interpolating sinc function is
$$p(x)={\sin(πx/h)\over (2π/h)\tan(x/2)},$$
where~$h=2π/N$.  This results in Toeplitz derivative matrices, with first columns
$$D_N=\pmatrix{0&-½\cot{1h\over 2}&½\cot{2h\over 2}&-½\cot{3h\over 2}&…}^{\rm T}$$
and
$$D_N^{(2)}=\pmatrix{-{π²\over 3h²}-{1\over6}&½\csc^2\left({1h\over 2}\right)&-½\csc^2\left({2h\over 2}\right)&…}^{\rm T}.$$
The matrices have the same (anti)-symmetry as the derivative operators.

For odd~$N$, the corresponding formulae (derived with the help of Matlab's symbolic toolbox) are
$$p(x)={\sin(πx/h)\over (2π/h)\sin(x/2)},$$
$$D_N=\pmatrix{0&-½\csc{1h\over 2}&½\csc{2h\over 2}&…}^{\rm T},$$
and
$$D_N^{(2)}=\pmatrix{-{π²\over 3h²}+{1\over12}&½\csc\left({1h\over 2}\right)\cot\left({1h\over 2}\right)&-½\csc\left({2h\over 2}\right)\cot\left({2h\over 2}\right)&…}^{\rm T}.$$

If the functions~$f:[0,2π]→{\bf R}$ and $g:[0,L]→{\bf R}$ have the same samples at a grid of~$N$ points on their respective domains, the derivatives will satisfy~$g^{(n)}(x)=(2π/L)ⁿf^{(n)}(x)$, and the derivative matrices scale in proportion.  Where $h$~occurs in the formulae above, it should be interpreted as~$h=2π/N$, whatever the actual grid step.

2019-07-10 SOP experiments

I've experimented with successive over-relaxation, in the script {\tt working2.jl}.  I won't include any plots for now, because I hope to make Jupyter notebooks or some kind of {\tt publish()} before long.

The first test is that I can solve~$Hψ=1·ψ₀$, where~$H$ is the 1D harmonic oscillator hamiltonian.  The known solution~$ψ₀$ is preserved by a single Gauss-Seidel step, and Gauss-Seidel converges geometrically, abeit slowly, with a residual around~$10^{-5}$ after 2000 steps.

Since the answer is known in this case, the optimum SOP extrapolation parameter can be determined as the least squares solution to~$(ψ'-ψ)a=(ψ₀-ψ)$.  This gives very large answers, around 165, but that is consistent with Gauss-Seidel taking 400 steps to gain each digit.  Supposedly SOP is only ever stable for~$0<a<2$, and I've found that it's unstable for~$a>1$ in my experiments.  Something is odd there.

With my current code, 2D Gauss-Seidel involves a~$10⁴×10⁴$ dense spectral derivative matrix, which isn't practical.  I know a way to rewrite it to exploit sparsity.  Imaginary time can evaluate the spectral derivatives by fourier transforms, and can do the 2D case.

Inspecting the residual~$∥(H-E)ψ∥$ is a plausible way to choose the SOP extrapolation parameter, just like it can be used to choose the imaginary time step.

2019-07-17 Progress on equilibrium order parameters

The results of my Gauss-Seidel and imaginary time simulations are in last week's progress report to Tapio.  (TODO: copy it here.)  Since then, I've tired imaginary time with projection.  Projecting the~$\exp(-τ∇²)$ operator, by applying it then renormalising the state, made adaptive convergence marginally faster.  Projecting the~$N(x)$ operator made it significantly slower.  So that was a failure.

The operator~$\exp(-τ∇²)$ is the solution to~$ψ_τ=-∇²ψ$.  I could do formally better by discretising in reciprocal space, solving the resulting system of ODEs with projection—the fourier transform is unitary—and applying the operator solution.  Since the speedup is marginal, that isn't worthwhile.

That leaves SOP.  Tapio suggests that the usual extrapolation parameter is fixed and around 1.4.  That isn't a massive speedup, so for now I'm going to stick with Gauss-Seidel.  I'm working in 1D for the moment, because the bookkeeping for fast 2D is a bit fiddly.

Gauss-Seidel solves the harmonic oscillator problem~$Hψ=E₀ψ₀$ reliably, albeit a bit slowly.  The next step is to solve $Hψ=E₀ψ$.  Choosing the initial value of~$ψ$ is tricky.  Obviously~$ψ₀$ is going to work, but I already know that.  (In the nonlinear problem, $ψ₀$~is a good choice, and I can see the solution converge to the repulsive order parameter.)

I've tried initialising~$ψ$ to white noise, and the results are interesting.  Most of the time, the solution converges as in~{\tt resp190717a.pdf}.  However, occasionally, an initial condition is drawn where it diverges as in~{\tt resp190717b.pdf}.  This is wierd.  If there is a parasitic solution, it should be excited by almost every random initial condition.  Maybe I can defer figuring that out until after I'm getting repulsive order parameters.  I can abort any run where the residual increases, and restart with a different random initial condition.

2019-07-18 Meeting Tapio

The speedup in SOR comes from extrapolating at each point in the grid, as you run through a Gauss-Seidel step.  Doing the whole step and then extrapolating is much slower.

For vortices, higher order finite differences are required to get accurate momentum near the vortex core.  Typically 19th order.  Why not go spectral with 150th order?  The boundary conditions are easier for finite differences.

Boundary conditions for vortices should be zero, periodic does wierd things with the phase at the edge of the domain.

The dynamics for the Bogoliubov~$u_j$ and~$v_j$ come from substituting any orthogonal modes into the mean-field Hamiltonian.  (Really?  Check that.) 

2019-07-19 Chris Billington's code

\item• The routine takes a {\tt boundary_mask} parameter, which excludes grid points from the update, so that their initial values serve as boundary conditions.  An $n+1$~point finite difference formula for~$D^{(n)}f$ is based on an~$n$th order polynomial.  Given~$f$, $Df$ through~$D^{(n-1)}f$ on the boundary, you have an~$n-1$th order Taylor polynomial.  Sampling this at~$n-1$ points outside the boundary plus one boundary point lets you solve for the first interior point.

Billington's SOR code has lots of complicated parallel stuff.  However, the basic algorithm is just Gauss-Seidel with pointwise extrapolation.

2019-07-19 Gauss-Seidel and eigenvalues

There's a subtlety which makes Gauss-Seidel good for solving nonlinear eigenproblems, but not for linear ones.

In the first instance, Gauss-Seidel solves a system of linear equations, $Ax=b$.  With a known eigenvalue~$λ₀$, the right-hand side can be iterated along with~$x$, and it turns out to solve~$Ax=λ₀x$ pretty well.  But there is a problem when $λ$~is unknown: if we guess~$λ$ wrong, the problem~$Ax=λx$ has no solution.  I don't know what Gauss-Seidel does in that case, but I'm guessing that it isn't pretty.

Nonlinearity makes it all easier!  Whatever guess is made for~$μ$, the problem~$L(x)x=μx$ has a solution.  That solution might not have the particle number that I want, but it still exists.  As the relaxation converges, I can adjust~$μ$ to get the number of particles that I want.

2019-07-29 Talking to Tapio

Circulation is quantised, not angular momentum.  If there is more than one vortex, different parts of the fluid have different angular momentum, and the average might not be a multiple of~$ℏ$.

How do you take an expectation value of a sound wave mode, which has a~$u$ and a~$v$ function?  Answer: treat~$u$ and~$v$ as wave functions, and take an average weighted by~$∥u∥²$ and~$∥v∥²$.  Apparently $∥u∥²$ is a kind of vacuum particle number.

For a non-interacting fluid in a harmonic trap, the energy of a vortex is~$ℏω_\perp$ per particle.

Sound wave modes at the boundary of the fluid can carry angular momentum.  In a rotating fluid, these get excited, then nonlinear interactions turn them into vortices.


\bye